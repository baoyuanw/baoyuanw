
<!DOCTYPE HTML>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta name="author" content="Yu Deng">
  <meta name="description" content="Yu Deng's Homepage">
  <meta name="keywords" content="Yu Deng,邓誉,homepage,主页,PhD,computer vision,MSRA,Tsinghua,3D face,3D reconstruction,image generation,implicit field">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Yu Deng (邓誉)'s Homepage</title>
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yu Deng (邓誉)</name>
              </p>
              <p style="text-align:center">
                Email: dengyu[at]xiaobing.ai &nbsp; &nbsp;&nbsp;&nbsp; <a href="https://scholar.google.com/citations?user=N3F3H0sAAAAJ&hl=en">Google Scholar</a> &nbsp; &nbsp;&nbsp;&nbsp;<a href="https://github.com/YuDeng">Github</a>
              </p>
              <p>I am currently a researcher at <a href="https://www.linkedin.com/company/xiaobing-ai">Xiaobing.ai</a>. My research interests include 3D reconstruction&generation, 3D representation learning, and neural rendering. 
              </p>
              <p>
                Before joining Xiaobing, I received my Ph.D. degree from <a href="https://www.ias.tsinghua.edu.cn/en/">Tsinghua University</a> under the supervision of <a href="https://www.microsoft.com/en-us/research/people/hshum/">Prof. Harry Shum</a> in 2022, and  
                worked closely with <a href="http://jlyang.org/">Jiaolong Yang</a> and <a href="https://www.microsoft.com/en-us/research/people/xtong/">Xin Tong</a> as a research intern at <a href="https://www.microsoft.com/en-us/research/group/visual-computing/">MSRA</a> from 2017 to 2022.  
	      Before that, I received my B.S. from Department of Physics in Tsinghua University in 2017.
              </p>
              <p>
                <strong>
                I'm seeking research interns on 3D-aware GAN and NeRF-based neural rendering. Feel free to send me an email if you are interested.
                </strong>
              </p>
		    
            </td>
            <td style="padding:15% 7% 7% 7%;width:40%;max-width:40%">
              <a href="images/yu.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/yu.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	
        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/graminverter.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Learning Detailed Radiance Manifolds for High-Fidelity and 3D-Consistent Portrait Synthesis from Monocular Image</papertitle>
                <br>
                <strong>Yu Deng</strong>, Baoyuan Wang, Heung-Yeung Shum
                <br>
                <em>arXiv</em> 2022,
                <br>
                <a href="https://arxiv.org/abs/2211.13901">[PDF]</a>
                <a href="https://yudeng.github.io/GRAMInverter/">[Project]</a>
		    <a href="images/graminverter.txt">[BibTeX]</a>
                <br>
                <p>We propose a learning-based approach for high-fidelity and 3D-consistent novel view synthesis of monocular portrait images.</p>
            </td>
        </tr>		

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/pdfgc.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis</papertitle>
                <br>
                Duomin Wang, <strong>Yu Deng</strong>, Zixin Yin, Heung-Yeung Shum, Baoyuan Wang
                <br>
                <em>arXiv</em> 2022,
                <br>
                <a href="https://arxiv.org/abs/2211.14506">[PDF]</a>
                <a href="https://dorniwang.github.io/PD-FGC/">[Project]</a>
                <br>
                <p>We propose a one-shot talking head synthesis approach with disentangled control over lip motion, eye gaze&blink, head pose, and emotional expression.</p>
            </td>
        </tr>		      
	      
		
        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/anifacegan.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>AniFaceGAN: Animatable 3D-Aware Face Image Generation for Video Avatars</papertitle>
                <br>
                Yue Wu, <strong>Yu Deng</strong>, Jiaolong Yang, Fangyun Wei, Qifeng Chen,  Xin Tong
                <br>
                <em>2022 Conference on Neural Information Processing Systems</em>, NeurIPS 2022,
		    <strong><font color="#FF8080">Spotlight</font></strong>
                <br>
                <a href="https://arxiv.org/abs/2210.06465">[PDF]</a>
                <a href="https://yuewuhkust.github.io/AniFaceGAN/">[Project]</a>
		    <a href="images/anifacegan.txt">[BibTeX]</a>
                <br>
                <p>We propose AniFaceGAN, an animatable 3D-aware GAN for multiview consistent face animation generation.</p>
            </td>
        </tr>		
		
        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/GRAMHD.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>GRAM-HD: 3D-Consistent Image Generation at High Resolution with Generative Radiance Manifolds</papertitle>
                <br>
                Jianfeng Xiang, Jiaolong Yang, <strong>Yu Deng</strong>, Xin Tong
                <br>
                <em>arXiv</em> 2022,
                <br>
                <a href="https://arxiv.org/abs/2206.07255">[PDF]</a>
                <a href="https://jeffreyxiang.github.io/GRAM-HD/">[Project]</a>
		    <a href="images/GRAM-HD.txt">[BibTeX]</a>
                <br>
                <p>We propose GRAM-HD, a 3D-aware GAN that can generate photorealistic and 3D-consistent images at 1024x1024 resolution.</p>
            </td>
        </tr>
	      
	<tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/gen_deform_nerf.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Generative Deformable Radiance Fields for Disentangled Image Synthesis of Topology-Varying Objects</papertitle>
                <br>
                Ziyu Wang, <strong>Yu Deng</strong>, Jiaolong Yang, Jingyi Yu, Xin Tong
                <br>
                <em>The 30th Pacific Graphics Conference</em>, PG 2022,
                <br>
                <a href="https://arxiv.org/abs/2209.04183">[PDF]</a>
		    <a href="https://ziyuwang98.github.io/GDRF/">[Project]</a>
		    <a href="https://github.com/ziyuwang98/GDRF/">[Code]</a>
		    <a href="images/GDRF.txt">[BibTeX]</a>
                <br>
                <p>We propose a generative model for synthesizing radiance fields of topology-varying objects with disentangled shape and appearance variations.</p>
            </td>
        </tr>

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/gram.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>GRAM: Generative Radiance Manifolds for 3D-Aware Image Generation</papertitle>
                <br>
                <strong>Yu Deng</strong>, Jiaolong Yang, Jianfeng Xiang, Xin Tong
                <br>
                <em>2022 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2022,
		    <strong><font color="#FF0000">Oral Presentation</font></strong>
                <br>
                <a href="https://arxiv.org/abs/2112.08867">[PDF]</a>
                <a href="https://yudeng.github.io/GRAM/">[Project]</a>
		<a href="https://github.com/microsoft/GRAM">[Code]</a>
				<a href="images/gram.txt">[BibTeX]</a>
                <br>
                <p>We propose Generative Radiance Manifolds (GRAM), a method that can generate 3D-consistent images with explicit camera control, trained on only unstructured 2D images.</p>
            </td>
        </tr>

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                <img src='images/arxiv_difnet.png' style="width:100%;max-width:100%; position: absolute;top: -5%">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Deformed Implicit Field: Modeling 3D Shapes with Learned Dense Correspondence</papertitle>
                <br>
                <strong>Yu Deng</strong>, Jiaolong Yang, Xin Tong
                <br>
                <em>2021 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2021,
                <br>
                <a href="https://arxiv.org/abs/2011.13650">[PDF]</a>
                <a href="https://github.com/microsoft/DIF-Net">[Code]</a>
				<a href="images/dif_net.txt">[BibTeX]</a>
                <br>
                <p>We propose a novel Deformed Implicit Field (DIF) representation for modeling 3D shapes of a category and generating dense correspondences among shapes with structure variations.</p>
            </td>
        </tr>
          
        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one" >
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/disco.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Disentangled and Controllable Face Image Generation via 3D Imitative-Contrastive Learning</papertitle>
                <br>
                <strong>Yu Deng</strong>, Jiaolong Yang, Dong Chen, Fang Wen, Xin Tong
                <br>
                <em>2020 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2020, 
				<strong><font color="#FF0000">Oral Presentation</font></strong>
                <br>
                <a href="https://arxiv.org/abs/2004.11660">[PDF]</a>
                <a href="https://github.com/microsoft/DiscoFaceGAN">[Code]</a>
				<a href="images/discoface.txt">[BibTeX]</a>
                <br>
                <p>We propose DiscoFaceGAN, an approach for face image generation of virtual people with disentangled, precisely-controllable latent representations for identity, expression, pose, and illumination. </p>
            </td>
        </tr>

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one" >
                    <img src='images/cvpr20_deep3dportrait.png' style="width:100%;max-width:100%; position: absolute;top: -5%">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Deep 3D Portrait from a Single Image</papertitle>
                <br>
                Sicheng Xu, Jiaolong Yang, Dong Chen, Fang Wen, <strong>Yu Deng</strong>, Yunde Jia, Xin Tong
                <br>
                <em>2020 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2020, 
                <br>
                <a href="https://arxiv.org/abs/2004.11598">[PDF]</a>
                <a href="https://github.com/sicxu/Deep3dPortrait">[Code]</a>
				<a href="images/deep3dportrait.txt">[BibTeX]</a>
                <br>
                <p>We propose a learning-based approach for recovering the 3D geometry of human head from a single portrait image without any ground-truth 3D data.</p>
            </td>
        </tr>

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one" >
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/accurate3d.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set</papertitle>
                <br>
                <strong>Yu Deng</strong>, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde Jia, Xin Tong
                <br>
                <em>2019 IEEE Conference on Computer Vision and Pattern Recognition Workshop on AMFG</em>, CVPRW 2019, 
                <strong><font color="#FF0000">Best Paper Award</font></strong>
                <br>
                <a href="https://arxiv.org/abs/1903.08527">[PDF]</a>
                <a href="https://github.com/microsoft/Deep3DFaceReconstruction">[Code]</a>
				<a href="images/accurate3d.txt">[BibTeX]</a>
                <br>
                <p>We propose a novel deep 3D face reconstruction approach that leverages a robust hybrid loss function and performs multi-image face reconstruction by exploiting complementary information from different images for shape aggregation.</p>
            </td>
        </tr>

        <tr></tr>
            <td style="padding:20px;width:0%;vertical-align:middle">
            </td>
            <td style="padding:20px;width:100%;vertical-align:middle">
		<hr style="margin-top:0px">
                <p>The website template was adapted from <a href="https://jonbarron.info/">Jon Barron</a>.</p>
            </td>
        </tr>

      </td>
    </tr>
  </table>
</body>

</html>
